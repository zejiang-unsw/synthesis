---
title: "synthesis: An R package for generating synthetic datasets"
output:
#  rmarkdown::html_vignette: default
  bookdown::html_vignette2: default
vignette: >
  %\VignetteIndexEntry{synthesis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/synthesis.bib
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
#options(rmarkdown.html_vignette.check_title = FALSE) # suppress un-needed warning
knitr::opts_chunk$set(
                      collapse = TRUE,
                      comment = "#>"
)

#library(tinytex) # to generate PDF output if you never use Latex before
# tinytex::install_tinytex()  # install TinyTeX
# tinytex:::is_tinytex()
```

```{r load-packages, include=FALSE}
# devtools::install_github("zejiang-unsw/WASP", dependencies = TRUE)
# devtools::install_github("zejiang-unsw/NPRED")
# devtools::install_github("zejiang-unsw/synthesis")
if(!require("lattice")) install.packages("lattice", repos = "http://cran.us.r-project.org", dependencies = c("Depends", "Imports", "Suggests"))
if(!require("forecast")) install.packages("forecast", repos = "http://cran.us.r-project.org", dependencies = c("Depends", "Imports", "Suggests"))

if(!require("WASP")) devtools::install_github("zejiang-unsw/WASP", dependencies = TRUE)
if(!require("NPRED")) devtools::install_github("zejiang-unsw/NPRED")


require(WASP)
require(NPRED)
require(Spectrum)

library(synthesis)
library(forecast)
library(ForecastComb)
```

# Introduction

The demand for synthetically generated data arises from the limited observed data in terms of both temporal and spatial coverage and the quality of these observed data. Observations are often contaminated by observational error, which can be mainly divided into two components: random error and systematic error [@Taylor1997]. Typically, the density function of a dataset recorded from a real-world system is complex and of an unknown shape and parameters, especially given noisy and sparse data along with inevitably instrumental error.  In addition, the demand of transparency to the public is growing while the need for confidentiality should be maintained since there are micro data containing sensitive information about individuals. Therefore, synthetic data is invaluable in both preserving privacy data and providing data with large quantity and high quality for the validation of the new methods or algorithms. 

According to @Surendra2017 and @Templ2017, there are diverse types of synthetic data generation approaches which can be generally categorized into three groups, including synthetic reconstruction (partially synthetic data), combinatorial optimization (hybrid synthetic data), and model-based generation (fully synthetic data). Synthetic reconstruction, one of the popular ways creating synthetic data is to extract relevant information (e.g., statistics like mean, standard deviation, or relationships between variables) in raw data, and reconstruct new, anonymous micro data with appropriate variables and granularity [@Caiola2010; @Cano2009; @Lundin2002]. However, using the partially synthetical generated data, the disclosure risk of sensitive information is high. The second approach simply combines both original and synthetic data, where sensitive information is replaced with synthetic generated data. The combinatorial optimization method has the advantages of both fully and partially synthetic data, which preserves privacy with high utility. However, this is achieved at a cost of more memory and processing time [@Surendra2017]. The model-based generation of synthetic data contains no original data and it is the most flexible method. Generally, the approach derives a model of the population (e.g., density function) from existing data and resamples from the derived model randomly. Multiple imputation and bootstrap methods are commonly used [@Ayala2016; @Reiter2009; @Schneider2015] but there are limitations, such as the difficulty of preserving the relationship among the variables as well as the requirement of large sample size to estimate parameters of the derived model reasonably. 

In this paper, we focus on the fully synthetic data generated by statistical models. It overcomes all the limitations as discussed above. The merits of the synthetic data generated by statistical models include: First, unlimited data length; Second, a dataset where true is known; Last, no issue of privacy protection. All of these ensure a fully reproducible research, and this type of dataset is very useful for validating various variable selection, regression and clustering algorithms. For example, variable and feature selection is an important step in the formation of data-driven models and how to validate if the adopted algorithm (e.g., the information theoretic based selection method) is applicable to a particular system is the first step [@Guyon2003]. As a result, the **synthesis** R-package is introduced here as an open-source synthetic data generator, which facilitates their applications and contributes to further assessments and improvements of their methods. The idea of generating synthetic data from statistical models and using it for algorithm assessments is not new, such as @Sharma2000, @Bowden2005, and @Galelli2014, however this is the first attempt to summarize and write them as a user friendly library. 

The remainder of the paper is organized as follows. First, we give an overview of how the **synthesis** package works. Second, all the available statistical models for generating synthetic data are listed and explained. In Section 4, the usage of synthesis package is illustrated under three different types of scenarios, including 1) the Partial Mutual Information (PMI) based variable selection; 2) the Neural Network-based time series forecasting; 3) the spectral clustering. The concluding remarks are made in the last section. 

# Overview of the synthesis package

In this section, we first introduce the idea of the **synthesis** package and demonstrate how to generate synthetic datasets in practice. The number of sample size (*N*) to be generated is mandatory and normally a large sample size will be adopted for the purpose of testing an given algorithm. Also, additional samples (e.g., *N*+500) are usually created to reduce the effect of an arbitrary initialization. The statistical models we included in the package so far have been widely used in many different fields by mimicking certain types of phenomenon in natural systems. For instance, @Lorenz1963 developed a simplified mathematical model for atmospheric convection in 1963, and the model is now known as the Lorenz equations, consisting of three ordinary differential equations. More details will be given in the following section. Here, we give an example how we can generate a synthetic dataset based on Lorenz system. 

```{r fig1, echo=TRUE, fig.cap='Example of Lorenz system: A 3D view', fig.height=7, fig.width=9, out.width='100%'}
#Synthetic example - Lorenz system
ts.l <- data.gen.Lorenz(sigma = 10, beta = 8/3, rho = 28, start = c(-13, -14, 47),
                        time = seq(0, 50, length.out = 1000))
summary(ts.l)

zlim <- c(floor(min(ts.l$z)),ceiling(max(ts.l$z)))
plot3D::scatter3D(ts.l$x,ts.l$y,ts.l$z, phi = 0, cex.lab=1.5, zlim=zlim,
                  col = "blue", ticktype = "detailed")
```

A synthetic dataset, as the name implies, is a collection of data generated programmatically. It often entails following characteristics: 

•	It can be numerical, binary, or categorical.

•	The number of features and the length of the dataset is unlimited. 

•	The underlying process is known and govern by the statistical model.

•	Different types and levels of noise can be introduced into the known system. 

In the following section, we lists all the statistical models available in the package with their governing mathematical equations. 

# Statistical models to generate synthetic datasets

## Basic statistical models

Among various statistical models, differencing and autoregressive models are remarkably flexible and capable of imitating a range of different time series patterns, such as stationary and autocorrelated data as well as nonstationary financial and economic data [@Hyndman2014].

#### Random walk model (RW)

The differenced series is the change between consecutive observations in the original series, and when the differenced series is white noise, it leads to the random walk model:
\begin{equation}
  \label{eq:1}
  {{x}_{t}}-{{x}_{t-1}}\text{ =}{{w}_{t}}
\end{equation}

The base model of the random walk with drift model [@Shumway2011] is given by,
\begin{equation}
  \label{eq:2}
  {{x}_{t}}\text{ =}\delta \text{ +}{{x}_{t-1}}\text{ +}{{w}_{t}}
\end{equation}
where $t= 1, 2, ..., n$ and ${{w}_{t}}$ is Gaussian white noise, ${{w}_{t}}\sim N(0,\sigma_w^{2})$. The constant $\delta$ is called the drift, and when $\delta =0$, the Equation \ref{eq:2} is called simply a random walk. The term random walk comes from the fact that when $\delta =0$, the value of the time series at time $t$ is the value of the series at time $t-1$ plus a completely random movement determined by ${{w}_{t}}$ [@Jiang2019]. Note that we may rewrite the Equation \ref{eq:2} as a cumulative sum of white noise variates. That is,
\begin{equation}
  \label{eq:3}
	{{x}_{t}}\text{ = }\delta t\text{ + }\sum\limits_{j=1}^{t}{{{w}_{t}}}
\end{equation}
From the rewritten Equation \ref{eq:3}, the drift $\delta$ in the model can be seen as the slope (or trend) of the time series. Therefore, this model is a good proxy for simulating trend, for example, the global temperature data.

#### Autoregressive model (AR)

Similar with multiple regression, autoregressive model, as their name suggests, is a regression with lagged values of the variable itself. The general expression of a $p$th-order autoregressive process can be written as [@Cryer2008]: 
\begin{equation}
	{{x}_{t}}=c+{{\phi }_{1}}{{x}_{t-1}}+{{\phi }_{2}}{{x}_{t-2}}+\cdots +{{\phi }_{p}}{{x}_{t-p}}+{{\varepsilon }_{t}}
\end{equation}

The following linear autoregressive model (AR) are given by @Sharma2000 and included in this package:
\begin{align}
	{{x}_{t}}=0.9{{x}_{t-1}}+0.866{{\varepsilon }_{t}}\\
	{{x}_{t}}=0.6{{x}_{t-1}}-0.4{{x}_{t-4}}+{{\varepsilon }_{t}}\\
	{{x}_{t}}=0.3{{x}_{t-1}}-0.6{{x}_{t-4}}-0.5{{x}_{t-9}}+{{\varepsilon }_{t}}
\end{align}
where $\epsilon$ is random Gaussian noise with zero mean and unit standard deviation. For each model, ${{x}_{t}}$ was arbitrarily initialized and a total number of $N+500$ data points were generated. The first 500 points were discarded to reduce any effects from the arbitrary initialization. These AR models are well studied particularly for validating variable selection algorithms [@Bowden2005; @Galelli2014; @Sharma2014]. It should be noted that variants of the random walk and autoregressive models introduced here can also be generated by the *arima.sim()* from the **stats** package [@R2020].

#### Threshold autoregressive model (TAR)

The basis of autoregressive models consists of linear models while in real-world most of systems are nonlinear. To overcome this shortcoming, threshold model is introduced, which is one of the most important types of nonlinear time series [@Tong2011]. Here, we show two examples of the nonlinear TAR models by @Sharma2000:
\begin{equation}
	{{x}_{t}}=
  \begin{cases}
     -0.9{{x}_{t-3}}+0.1{{\varepsilon }_{t}} & if\text{ }{{x}_{t-3}}\le 0  \\
     0.4{{x}_{t-3}}+0.1{{\varepsilon }_{t}} & if\text{ }{{x}_{t-3}}>0
  \end{cases}
\end{equation}

\begin{equation}
  {{x}_{t}}=
  \begin{cases}
     0.5{{x}_{t-6}}-0.5{{x}_{t-10}}+0.1{{\varepsilon }_{t}} & if\text{ }{{x}_{t-6}}\le 0  \\
     0.8{{x}_{t-10}}+0.1{{\varepsilon }_{t}} & if\text{ }{{x}_{t-6}}\le 0
  \end{cases} 
\end{equation}
where $\epsilon$ is random Gaussian noise with zero mean and unit standard deviation. Similarly, for each model, ${{x}_{t}}$ was arbitrarily initialized and a total number of $N+500$ data points were generated. The first 500 points were discarded to reduce any effects from the arbitrary initialization. The nonlinear TAR models presented here are two variations from the group of two-regime threshold autoregressive model [@Cryer2008]. However, in the synthesis package we have also included the generalized version for TAR model, and an example from @Jiang2020 is generated as follows. 

```{r mod1, fig.cap=c('Example of Random walk model','Example of Autoregressive models','Example of Threshold autoregressive models'), fig.height=c(5,7,7), fig.width=c(5,9,9), out.width= c('80%','100%','100%')}
set.seed(154)
sample=200
###Synthetic example - RW model
data.rw <- data.gen.rw(nobs=sample,drift=0.2,sd=1)

plot.ts(data.rw$xd, ylim=c(-5,55), main="Random walk", xlab=NA, ylab=NA, cex.axis=1.5)
lines(data.rw$x, col=4); abline(h=0, col=4, lty=2); abline(a=0, b=.2, lty=2)
###Synthetic example - AR models
ar1 <- data.gen.ar1(nobs=sample)$x
ar4 <- data.gen.ar4(nobs=sample)$x
ar9 <- data.gen.ar9(nobs=sample)$x

zoo::plot.zoo(cbind(ar1,ar4,ar9), col=c("black","red","blue"), ylab=c("AR1","AR4","AR9"),
              main=NA, xlab=NA, cex.axis=1.5)
###Synthetic example - TAR models
tar1 <- synthesis::data.gen.tar1(nobs=1000)$x #TAR in Equation (8)
tar2 <- synthesis::data.gen.tar2(nobs=1000)$x #TAR in Equation (9)

# Generalized TAR, a example in Jiang et al. (2020)
tar  <- synthesis::data.gen.tar(nobs=1000,ndim=9,phi1=c(0.6,-0.1),
                                phi2=c(-1.1,0),theta=0,d=2,p=2,noise=0.1)$x 

zoo::plot.zoo(cbind(tar1,tar2,tar), col=c("black","red","blue"), ylab=c("TAR1","TAR2","TAR"),
              main=NA, xlab=NA, cex.axis=1.5)
```

## Nonlinear systems

There are many other types of nonlinear systems, for example, a set of equations where the unknows appear as variables of a polynomial of degree higher than one. Here, we focus on more complicated statistical models which have been widely used for benchmark dataset generation.

#### Hysteresis loop (HL)

Hysteresis is a common nonlinear phenomenon in natural systems, for example, storage-discharge relationships in rivers systems [@Fovet2015]. We adopt an analytical model to approximate a classical hysteresis loop [@Lapshin1995], and its general transcendental formulation can be described as follows:
\begin{equation}
\begin{cases}
  {{x}_{t}}=a\cos (2\pi ft)+s{{\varepsilon }_{t}} \\ 
  {{y}_{t}}=b\cos {{(2\pi ft)}^{m}}-c\sin {{(2\pi ft)}^{n}}+s{{\varepsilon }_{t}}
\end{cases}
\end{equation}
where $a$, $b$ and $c$ are parameters, $m$ and $n$ are integer numbers, and s is a scaling factor used to alter the levle of noise in the output, which all together specify the classical hysteresis loop. The default HL model datasets was generated from $y_t$ with $f = 25Hz$, and additional nine candidate predictors were generated with various frequencies. The default values of the system parameters are $a = 0.8$, $b = 0.6$, and $c = 0.2$, which is known to produce a deterministic nonlinear time series. As an example, one formulation of the synthetic data from @Jiang2020 is shown here.

#### Friedman

Friedman datasets are often used as benchmark datasets for machine learning and have been achieved by some online open databases, like WEKA [@Hall2009] and DELVE [@Rasmussen1996] repositories. The Friedman family of datasets is generated by the following equation, suggested by @Friedman1991: 
\begin{equation}
	y=10\sin (\pi {{x}_{1}}{{x}_{2}})+20\sin {{({{x}_{3}}-0.5)}^{2}}+10{{x}_{4}}+5{{x}_{5}}+s\varepsilon
\end{equation}
where $s$ is a scaling factor used to alter the level of noise in the output. Variate, $x_i$, is  sampled from a uniform distribution, $x\sim U(0,1)$, for all $i = 1, ..., 5$. In the original formulation, 10-dimension inputs $x$ are generated while only first five inputs are relevant with the response. Additionally, datasets can be generated with both zero and various degrees of collinearity. The 10 candidate inputs were generated from correlated uniform variates according to the method by @Fackler1999. In each generated dataset, additional 500 data points were discarded so as to reduce the effect of an arbitrary initialization. In this paper, we show two sample datasets generated with independent and correlated uniform variates, respectively.

```{r mod2, fig.cap=c('Example of Hysteresis loop', 'Example of Friedman with independent and correlated uniform variates'), fig.height=c(4,7), fig.width=c(4,9), out.width= c('80%','100%')}
sample=1000
###synthetic example - Hysteresis loop
#frequency, sampled from a given range
fd <- c(3,5,10,15,25,30,55,70,95)
data.HL <- data.gen.HL(m=3,n=5,nobs=sample,fp=25,fd=fd, sd.x=0, sd.y=0)

plot(data.HL$x,data.HL$dp[,data.HL$true.cpy], xlab="x", ylab = "y", type = "p",
     cex.axis=1.5,cex.lab=1.5)
###synthetic example - Friedman
#Friedman with independent uniform variates
data.fm1 <- data.gen.fm1(nobs=sample, ndim = 9, noise = 0)
#Friedman with correlated uniform variates
data.fm2 <- data.gen.fm2(nobs=sample, ndim = 9, r = 0.6, noise = 0) 

zoo::plot.zoo(cbind(data.fm1$x,data.fm2$x), col=c("red","blue"), main=NA, xlab=NA,
              ylab=c("Friedman with \n independent uniform variates",
                     "Friedman with \n correlated uniform variates"))
```

## Dynamic systems

The study of dynamical systems is the focus of dynamical systems theory (also known as nonlinear dynamics and chaos theory), which has applications to a wide variety of fields such as  physics, biology, chemistry, and engineering [@Strogatz2000]. They often consists of methods for analyzing differential equations and iterated mappings. In this package, we have included five classical dynamical systems, including Hénon map, Logistic map, and Duffing map as well as Rössler system and Lorenz system. It is noted that the dynamical systems mentioned above can also be reproduced by the functions in package of @Garcia2020. 

#### Hénon map 

The Hénon map devised by @Henon1976 is a two-dimensional map used to illuminate microstructure of strange attractors. It is one of the most studied examples of dynamical systems that exhibit chaotic behavior, which can be written as: 
\begin{equation}
\begin{cases}
  {{x}_{n+1}}=1-ax_{n}^{2}+{{\theta }_{n}} \\ 
  {{\theta }_{n+1}}=b{{x}_{n}}
\end{cases}
\end{equation}
The map depends on two parameters, $a$ and $b$, which for the classical Hénon map have values of $a = 1.4$ and $b = 0.3$. For the classical values, the Hénon map is chaotic. For other values of $a$ and $b$ the map may be chaotic, intermittent, or converge to a periodic orbit. The initial condition of this system is randomly generated from a uniform distribution ranging from $(-0.5, 0.5)$, and similarly the first 500 points were discarded so as to reduce the effect of an arbitrary initialization. 

#### Logistic  map 

The logistic map, introduced by @Verhulst1845 and promoted by @May1976, is a discrete-time analog of the logistic equation for population growth. This is an illustrative example of how complex, chaotic behavior can arise from very simple non-linear dynamical equations. The Logistic map can be written as [@Strogatz2000]: 
\begin{equation}
	{{x}_{n+1}}=r{{x}_{n}}(1-{{x}_{n}})
\end{equation}
where $r$ represents the growth rate and the value of the control parameter $r$ is restricted to the range of $[0, 4]$. The initial condition of this system is randomly generated from a uniform distribution ranging from $(0, 1)$, and the first 500 points were discarded in order to reduce the effect of an arbitrary initialization. 

#### Duffing  map 

A Duffing map, also known as Holmes map, can be expressed as [@Dignowity2013]:
\begin{equation}
\begin{cases}
  {{x}_{n+1}}={{\theta }_{n}} \\ 
  {{\theta }_{n+1}}=-\beta {{x}_{n}}+\alpha {{\theta }_{n}}-\theta _{n}^{3}
\end{cases}
\end{equation}
This map depends on two constant parameters $\alpha$ and $\beta$, usually set to 2.75 and 0.15, respectively. The initial condition of this system is randomly generated from a uniform distribution ranging from $(-0.5, 0.5)$. Similarly, the first 500 points were discarded so as to reduce the effect of an arbitrary initialization.

Illustrations of these three iterated mappings in their phase portraits using the **synthesis** package are given below.

```{r mod3, fig.cap=c('Example of Hénon map','Example of Logistic map','Example of Duffing map'), fig.height=4, fig.width=9, out.width= '100%'}
###Synthetic example - Iterated mappings
par(mfrow=c(1,3), ps=12, cex.lab=1.5, pty="s")
Henon.map=data.gen.Henon(nobs = 1000, a = 1.4, b = 0.3, 
                         start = runif(min = -0.5, max = 0.5, n = 2), do.plot=TRUE)
Logistic.map=data.gen.Logistic(nobs = 1000, r=4, 
                               start=runif(n = 1, min = 0, max = 1), do.plot=TRUE)
Duffing.map=data.gen.Duffing(nobs = 1000, a = 2.75, b = 0.2, 
                             start = runif(min = -0.5, max = 0.5, n = 2), do.plot=TRUE)
```

#### Rössler system

A continuous dynamical system with a simple strange attractor was proposed by @Rossler1976, which was inspired by a taffy-pulling machine. The Rössler system consists of a set of three differential equations defined as follows:
\begin{equation}
\begin{cases}
  \dot{x}=-y-z, \\ 
  \dot{y}=x+ay, \\ 
  \dot{z}=b+z(x-c).
\end{cases}
\end{equation}
where $a$, $b$ and $c$ are parameters that specify the chaotic system. We investigate the system parameters with parameters value $a = 0.2$, $b = 0.2$, and $c = 5.7$, which is known to produce a deterministic chaotic time series [@Harrington2017]. In default setting, the time range is fixed from 0 to 50, and a total number of $N=1000$ paired observations $(x_t, y_t, z_t)$ were generated from an initial condition of $(-2, -10, 0.2)$ with no white noise.

```{r mod4, fig.cap='Example of Rossler system: Phase portraits in a 2D projection of its state space', fig.height=5, fig.width=9, out.width= '100%'}
###Synthetic example - Rossler
ts.r <- synthesis::data.gen.Rossler(a = 0.2, b = 0.2, w = 5.7, start=c(-2, -10, 0.2),
                         time = seq(0, 50, length.out = 1000), s=0)

par(mfrow=c(1,2), ps=12, cex.lab=1.5)
plot(ts.r$x,ts.r$y, xlab="x",ylab = "y", type = "l")
plot(ts.r$x,ts.r$z, xlab="x",ylab = "z", type = "l")
```

#### Lorenz system

The Lorenz system is a system of ordinary differential equations first introduced by Lorenz (1963), which provids a simplified mathematical model for atmospheric convection. It is well known for having chaotic solutions for certain parameter values and initial conditions. The model consists of three ordinary differential equations defined as:
\begin{equation}
\begin{cases}
  \dot{x}=\sigma (y-x), \\ 
  \dot{y}=x(\rho -z)-y, \\ 
  \dot{z}=xy-\beta z.
\end{cases}
\end{equation}
where $\sigma$, $\rho$, $\beta$>0 are parameters. The default values for the system parameters are $\sigma=10$, $\rho=28$, $\beta=8/3$ and the time range is fixed from 0 to 50, and a total number of $N=1000$ paired observations $(x_t, y_t, z_t)$ were generated from an initial condition of $(-13, -14, 47)$ with no white noise.


```{r mod5, fig.cap='Example of Lorenz system: Phase portraits in a 2D projection of its state space', fig.height=5, fig.width=9, out.width= '100%'}
###Synthetic example - Lorenz
ts.l <- data.gen.Lorenz(sigma = 10, beta = 8/3, rho = 28, start = c(-13, -14, 47),
                        time = seq(0, 50, length.out = 1000), s=0)

par(mfrow=c(1,2), ps=12, cex.lab=1.5)
plot(ts.l$x,ts.l$y, xlab="x",ylab = "y", type = "l")
plot(ts.l$x,ts.l$z, xlab="x",ylab = "z", type = "l")
```

## Classification systems

The statistical models we have discussed above are mainly used for regression problem generation. However, there is another group of problems which is related to classification and clustering.  In this section, we will focus on three typical classification-oriented data generation, including blobs, circles, and spirals. It is worth noting that scikit-learn in python have the similar data generation functions @Pedregosa2011, and the functions provided here can be regarded as implementations in R environment. 

#### Blobs

One of the interesting clusters is the Gaussian blobs, which represents several real-world phenomenons, for instance, in biology clustering a single omic data platform [@Rappoport2018]. This model is based on Gaussian mixture model, where each component is a D-variate Gaussian density functions [@Reynolds2009]:
\begin{equation}
p(\mathbf{x}|{{\mu }_{i}},{{\mathbf{\Sigma }}_{i}})=\frac{1}{{{(2\pi )}^{D/2}}|{{\mathbf{\Sigma}}_{i}}{{|}^{1/2}}}\exp \left\{ -\frac{1}{2}{{(\mathbf{x}-{{\mu }_{i}})}^{T}}\;{{\mathbf{\Sigma}}_{i}^{-1}}\;(\mathbf{x}-{{\mu }_{i}}) \right\}
\end{equation}
where $\mu_i$ is the mean vector and $\Sigma_i$ is covariance matrix. The idea is to place “blobs” of probability mass in the space to cover the data well. 

#### Circles 

Circle is a ring-shaped object, and is one of the most common classes in our daily life. It can be easily produced by the parametric form of equations using the trigonometric functions sine and cosine as: 
\begin{equation}
\begin{matrix}
  x=a+r\cos t \\ 
  y=b+r\sin t
\end{matrix}
\end{equation}
where $t$ is a parametric variable in the range of 0 to $2\pi$. Gaussian noise can be added to each data point generated by the function of the **synthesis** package.

#### Spirals

Spirals are very common in the shells of snails and nautilus, and they are often used as synthetic datasets in the clustering problem. Compared with blobs, both circles and spirals are non-Gaussian clusters, and are relatively difficult to be classified. A spiral can be easily expressed as: 
\begin{equation}
\begin{matrix}
  x=r(\varphi )\cos \varphi \\ 
  y=r(\varphi )\sin \varphi 
\end{matrix}
\end{equation}
where $r$ is a monotonic continuous function of angle $\varphi$. 

Examples of datasets generated by these three classification-based functions in the **synthesis** package are provided below.

```{r class, fig.cap='Example of Classification system: Blobs, Circles and Spirals', fig.height=4, fig.width=9, out.width= '100%'}
set.seed(2020)
sample=500

par(mfrow=c(1,3), ps=12, cex.lab=1.5, pty="s")
Blobs=data.gen.blobs(nobs=sample, features=2, centers=3, sd=1, bbox=c(-10,10), do.plot=TRUE)
Circles=data.gen.circles(n = sample, r_vec=c(1,2), start=runif(1,-1,1), s=0.1, do.plot=TRUE)
Spirals=data.gen.spirals(n = sample, cycles=2, s=0.01, do.plot=TRUE)
```

# Demonstration of the usage of the synthesis package 

The **synthesis** package is mainly designed for testing new algorithms. Here, we present three scenarios where the Partial Mutual Information (PMI) based variable selection algorithm, the Neural Network-based time series forecasting algorithm, and the spectral clustering algorithm are tested against the different synthetic datasets generated by the package. 

### Input Variable selection

Variable and feature selection plays an important role in data-driven models, which aims to find the most relevant variables. With variable and feature selection, a subset of variables is identified as the input of a model. Here, we test the PMI based input variable selection algorithm against the AR models. 

```{r demo1, eval=TRUE}
set.seed(202006)
sample=500

data.ar1 <- data.gen.ar1(nobs=sample)
data.ar4 <- data.gen.ar4(nobs=sample)
data.ar9 <- data.gen.ar9(nobs=sample)
#-----------------------------------------------
NPRED::stepwise.PIC(data.ar1$x, data.ar1$dp)
NPRED::stepwise.PIC(data.ar4$x, data.ar4$dp)
NPRED::stepwise.PIC(data.ar9$x, data.ar9$dp)
```

### Time series forecasting

Forecasting is applied in many fields: short-term weather forecasting, long-term drought forecasting, daily stock market forecasting, etc. No matter under what circumstances or time scales, forecasting is an important aid to effective and efficient planning [@Hyndman2014]. Time series forecasting is one of the most common types of forecasting. In this paper, an example of wavelet-based method [@Jiang2020] and artificial neural network (ANN) coupled algorithm is tested with the nonlinear dataset generated from Rössler system. 

```{r demo2, fig.height=4, fig.width=9, out.width= '100%'}
set.seed(2020)
sample=300
h = 12*2 # prediction horizon
ts.r <- synthesis::data.gen.Rossler(a = 0.2, b = 0.2, w = 5.7, start=c(-2, -10, 0.2),
                         time = seq(0, 50, length.out = sample), s=0.1)

n=sample-h; wf="haar"
if(wf!="haar") v <- as.integer(readr::parse_number(wf)/2) else v <- 1
if(wf=="haar") J <- ceiling(log(n/(2*v-1))/log(2))-1 else J <- ceiling(log(n/(2*v-1))/log(2))

#variance transformation
data <- list(x=ts.r$z[1:(sample-h)], dp=cbind(ts.r$x[1:(sample-h)],ts.r$y[1:(sample-h)]))
dwt <- WASP::modwt.vt(data, wf, J, boundary = "periodic")
#-----------------------------------------------------------
y <- ts.r$z
xreg <- dwt$dp.n
NForecast=h
if(TRUE){
  n1 <- length(y)-NForecast
  train_o <- y[1:n1]
  test_o <- y[-c(1:n1)]
  
  xreg <- as.matrix(xreg)
  ndim <- ncol(xreg)
  
  AllForecast <- NULL;AllPrediction <- NULL
  #---------------------------------------------------------
  for(i in 1:ndim){

    ANNFit <- forecast::nnetar(y=as.ts(train_o), xreg = xreg[1:n1,i])
    fit.xreg <- auto.arima(xreg[1:n1,i])
    ANNPredict <- as.matrix(ANNFit$fitted)
    
    ANNForecast <- forecast::forecast(ANNFit,xreg=forecast(fit.xreg,h=NForecast)$mean, h=NForecast)

    AllPrediction <- cbind(AllPrediction,ANNPredict)
    AllForecast <- cbind(AllForecast,as.matrix(ANNForecast$mean))
  }
  #---------------------------------------------------------
  # Forecast combination                                      
  data.comb <- foreccomb(train_o, AllPrediction, newobs=test_o, newpreds=AllForecast)
  data.comb$Forecasts_Train <- ts(data.comb$Forecasts_Train)
  comb.fit <- do.call("comb_BG",list(data.comb))
}

Accuracy.Train = comb.fit$Accuracy_Train
Accuracy.Test = comb.fit$Accuracy_Test

Finalforecast=comb.fit$Forecasts_Test
FinalPrediction=comb.fit$Fitted

plot(1:sample, y, col="black", lwd=2, type="l", xlab=NA)
lines(1:(sample-h), FinalPrediction, col="red",lwd=1)
lines((sample-h+1):sample, Finalforecast, col="blue",lwd=1)
abline(v = 276, col="grey", lwd=3, lty=2)
```

### Spectral clustering

Clustering plays an important role in machine learning, which can segment data into appropriate groups and conduct exploratory analysis. Two most popular algorithms are K-means and hierarchical clustering, but in this paper we are going to test the emerging algorithm, spectral clustering [@John2019]. 

```{r demo3, eval=TRUE, fig.keep='last', fig.height=5, fig.width=9, out.width= '100%'}
set.seed(2020)
sample=500
Blobs=data.gen.blobs(nobs=sample, features=2, centers=3, sd=1, bbox=c(-10,10), do.plot=FALSE)
Circles=data.gen.circles(n = sample, r_vec=c(1,2), start=runif(1,-1,1), s=0.1, do.plot=FALSE)
Spirals=data.gen.spirals(n = sample, cycles=2, s=0.01, do.plot=FALSE)

#data = data.frame(Blobs$x, classes=Blobs$classes)
data = data.frame(Circles$x, classes=Circles$classes)
#data = data.frame(Spirals$x, classes=Spirals$classes)

df = data.frame(t(data[,-3]))
cluster.spec <- Spectrum::Spectrum(df,method=2,showres=FALSE,tunekernel=FALSE)

par(mfrow=c(1,2), ps=12, cex.lab=1.5)
plot(data[,-3], xlab = NA, ylab = NA, main = "obs", col=data[,3], type = "p")
plot(data[,-3], xlab = NA, ylab = NA, main = "sim", col=cluster.spec$assignments, type = "p")
```

# Summary
There is hardly any package in R providing such a diverse functions for generating synthetic datasets. These generated fully synthetic datasets have the advantages of known truth, unlimited data length and free of data privacy issue, and they can be widely used for variable selection, regression, and classification problem generation. Accordingly, in this paper we have presented three applications using the **synthesis** package. There is no doubt that the number of statistical models involved is limited and certainly this could be planed for future development.

\bibliography{RJreferences}
